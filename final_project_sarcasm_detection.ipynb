{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "final_project_sarcasm_detection.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTwHgK5n_Q8S"
      },
      "source": [
        "!pip install sentencepiece\n",
        "!pip install transformers\n",
        "from transformers import XLMRobertaTokenizer, XLMRobertaModel\n",
        "import torch\n",
        "\n",
        "import json\n",
        "import pandas as pd\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "def parse_data(file):\n",
        "    for l in open(file,'r'):\n",
        "        yield json.loads(l)\n",
        "\n",
        "data = list(parse_data('data.json'))\n",
        "headlines = []\n",
        "\n",
        "for i in (data):\n",
        "  headlines.append(i['headline'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a7JXoQ7M3L22",
        "outputId": "17bfcfa8-3be4-4b76-e1b6-babfa5506fa5"
      },
      "source": [
        "\n",
        "tokenizer = XLMRobertaTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
        "model = XLMRobertaModel.from_pretrained(\"xlm-roberta-base\")\n",
        "#print(tokenizer)\n",
        "inputs = tokenizer(\"Hello world!\", return_tensors=\"pt\")\n",
        "outputs = model(**inputs)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaModel: ['lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
            "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q14n40NKFduD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f8d33c7-2008-47f1-fd60-d57dd7dff78d"
      },
      "source": [
        "df = pd.DataFrame(data)\n",
        "df['model_truth_values'] = df['is_sarcastic'].apply(lambda input: [0,1] if input == 1 else [1,0])\n",
        "# df['is_sarcastic'].apply(lambda input: [0,1] if input == 1 else [1,0])\n",
        "df['model_truth_values']"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0        [0, 1]\n",
              "1        [1, 0]\n",
              "2        [1, 0]\n",
              "3        [0, 1]\n",
              "4        [0, 1]\n",
              "          ...  \n",
              "28614    [0, 1]\n",
              "28615    [0, 1]\n",
              "28616    [1, 0]\n",
              "28617    [0, 1]\n",
              "28618    [0, 1]\n",
              "Name: model_truth_values, Length: 28619, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "Koui6FCd0k_S",
        "outputId": "f4355892-baeb-4829-f2d0-43e8734fa656"
      },
      "source": [
        "df"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>is_sarcastic</th>\n",
              "      <th>headline</th>\n",
              "      <th>article_link</th>\n",
              "      <th>model_truth_values</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>thirtysomething scientists unveil doomsday clo...</td>\n",
              "      <td>https://www.theonion.com/thirtysomething-scien...</td>\n",
              "      <td>[0, 1]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>dem rep. totally nails why congress is falling...</td>\n",
              "      <td>https://www.huffingtonpost.com/entry/donna-edw...</td>\n",
              "      <td>[1, 0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>eat your veggies: 9 deliciously different recipes</td>\n",
              "      <td>https://www.huffingtonpost.com/entry/eat-your-...</td>\n",
              "      <td>[1, 0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>inclement weather prevents liar from getting t...</td>\n",
              "      <td>https://local.theonion.com/inclement-weather-p...</td>\n",
              "      <td>[0, 1]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>mother comes pretty close to using word 'strea...</td>\n",
              "      <td>https://www.theonion.com/mother-comes-pretty-c...</td>\n",
              "      <td>[0, 1]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28614</th>\n",
              "      <td>1</td>\n",
              "      <td>jews to celebrate rosh hashasha or something</td>\n",
              "      <td>https://www.theonion.com/jews-to-celebrate-ros...</td>\n",
              "      <td>[0, 1]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28615</th>\n",
              "      <td>1</td>\n",
              "      <td>internal affairs investigator disappointed con...</td>\n",
              "      <td>https://local.theonion.com/internal-affairs-in...</td>\n",
              "      <td>[0, 1]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28616</th>\n",
              "      <td>0</td>\n",
              "      <td>the most beautiful acceptance speech this week...</td>\n",
              "      <td>https://www.huffingtonpost.com/entry/andrew-ah...</td>\n",
              "      <td>[1, 0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28617</th>\n",
              "      <td>1</td>\n",
              "      <td>mars probe destroyed by orbiting spielberg-gat...</td>\n",
              "      <td>https://www.theonion.com/mars-probe-destroyed-...</td>\n",
              "      <td>[0, 1]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28618</th>\n",
              "      <td>1</td>\n",
              "      <td>dad clarifies this not a food stop</td>\n",
              "      <td>https://www.theonion.com/dad-clarifies-this-no...</td>\n",
              "      <td>[0, 1]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>28619 rows Ã— 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       is_sarcastic  ... model_truth_values\n",
              "0                 1  ...             [0, 1]\n",
              "1                 0  ...             [1, 0]\n",
              "2                 0  ...             [1, 0]\n",
              "3                 1  ...             [0, 1]\n",
              "4                 1  ...             [0, 1]\n",
              "...             ...  ...                ...\n",
              "28614             1  ...             [0, 1]\n",
              "28615             1  ...             [0, 1]\n",
              "28616             0  ...             [1, 0]\n",
              "28617             1  ...             [0, 1]\n",
              "28618             1  ...             [0, 1]\n",
              "\n",
              "[28619 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76e_UJIvFwpe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3257be1a-ee9f-4a7f-81bc-837d37bdf77a"
      },
      "source": [
        "df.is_sarcastic"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0        1\n",
              "1        0\n",
              "2        0\n",
              "3        1\n",
              "4        1\n",
              "        ..\n",
              "28614    1\n",
              "28615    1\n",
              "28616    0\n",
              "28617    1\n",
              "28618    1\n",
              "Name: is_sarcastic, Length: 28619, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUdYMfdXvNwt"
      },
      "source": [
        "def get_BERT_input(headlines, tokenizer):\n",
        "  # Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "  input_ids = []\n",
        "  attention_masks = []\n",
        "  encoded_dict = tokenizer.batch_encode_plus(\n",
        "                      headlines,                      # Sentence to encode.\n",
        "                      add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                      max_length = 65,           # Pad & truncate all sentences.\n",
        "                      pad_to_max_length = True,\n",
        "                      return_attention_mask = True,   # Construct attn. masks.\n",
        "                      return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                )\n",
        "  input_ids, attention_mask = encoded_dict['input_ids'], encoded_dict['attention_mask']\n",
        "  # Add the encoded sentence to the list.    \n",
        "  input_ids = encoded_dict['input_ids']\n",
        "  \n",
        "  # And its attention mask (simply differentiates padding from non-padding).\n",
        "  attention_mask = encoded_dict['attention_mask']\n",
        "  return torch.tensor(input_ids), torch.tensor(attention_mask)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-biS3igJvj3K",
        "outputId": "cce5ed85-e9b0-45e9-f596-af9bbbb6c61a"
      },
      "source": [
        "get_BERT_input([\"Ani draw eyeliner today\",\"It is very hot today\"],tokenizer)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2110: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[    0, 32340, 79442, 36298, 79852, 18925,     2,     1,     1,     1,\n",
              "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "              1,     1,     1,     1,     1],\n",
              "         [    0,  1650,    83,  4552,  8010, 18925,     2,     1,     1,     1,\n",
              "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "              1,     1,     1,     1,     1]]),\n",
              " tensor([[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "         [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sfPyxpxExyc_"
      },
      "source": [
        "# Dataset"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9YzPtfVqyEmm"
      },
      "source": [
        "\n",
        "class SatcasmDataset(Dataset):\n",
        "    def __init__(self, df):\n",
        "        self.headlines = df[\"headline\"]\n",
        "        self.model_truth_values = df[\"model_truth_values\"]\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.headlines)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \n",
        "        return self.headlines.iloc[idx], torch.tensor(self.model_truth_values.iloc[idx])"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_1KFQzQ24yS"
      },
      "source": [
        "dataset = SatcasmDataset(df)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kScf-EHM3Mz8",
        "outputId": "d2cee927-1647-4a80-e2c0-8a338c687583"
      },
      "source": [
        "dataset[50]"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('lgbt christians speak out: \"love the sinner, hate the sin\" won\\'t cut it anymore',\n",
              " tensor([1, 0]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j2CLHc8D3zyH"
      },
      "source": [
        "train_dataloader = DataLoader(dataset, batch_size=32, shuffle=True)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lla3GNdY_DX-"
      },
      "source": [
        "Pythorch model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YPL1luIy-8vt"
      },
      "source": [
        "from torch import nn\n",
        "\n",
        "class SarcasmModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SarcasmModel, self).__init__()\n",
        "        self.XLM = tokenizer = XLMRobertaModel.from_pretrained(\"xlm-roberta-base\")\n",
        "        self.hidden_1 = nn.Linear(768,2)\n",
        "        self.softmax = nn.Softmax(dim = 1)\n",
        "        self.to_delete = 2\n",
        "        \n",
        "\n",
        "    def forward(self, b_input_ids, b_attention_mask):\n",
        "        bert_output = self.XLM(b_input_ids, b_attention_mask)\n",
        "        hidden_state = bert_output['last_hidden_state']\n",
        "        sentence_vector = torch.mean(hidden_state, dim =  1)\n",
        "        x = self.hidden_1(sentence_vector)\n",
        "        probabilities = self.softmax(x)\n",
        "        return probabilities\n",
        "\n",
        "\n"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CdW0gbn_Jubk",
        "outputId": "0c0f7ad0-e6a5-4a0e-fdee-3de4cbef5031"
      },
      "source": [
        "from torch.optim import AdamW\n",
        "model = SarcasmModel()\n",
        "model = model.to(device)\n",
        "loss_function = nn.BCELoss().to(device)\n",
        "optimizer = AdamW(model.parameters(), lr = 2e-5, eps = 1e-8)\n",
        "batch_size = 32\n",
        "for i_batch, (b_headlines, b_model_truth_values) in enumerate(train_dataloader):\n",
        "  # print(step)\n",
        "  # print((b_headlines[0]))\n",
        "  # print((b_model_truth_values[0]))\n",
        "  optimizer.zero_grad()\n",
        "  b_input_ids, b_attention_mask = get_BERT_input(b_headlines,tokenizer)\n",
        "  b_prediction = model(b_input_ids.to(device),b_attention_mask.to(device))\n",
        "  loss = loss_function(b_prediction.to(device),b_model_truth_values.float().to(device))\n",
        "  \n",
        "  if i_batch % 10 == 0:\n",
        "      iteration = i_batch*batch_size\n",
        "      print(\"Iteration:\", i_batch*batch_size, \"Loss:\", loss.data)\n",
        "      batch_accuracy = torch.mean(torch.sum(b_prediction * b_model_truth_values.to(device), dim=1))\n",
        "      print(\"Batch Accuracy:\", batch_accuracy.data*100)\n",
        "      if iteration % 5120 == 0:\n",
        "        # torch.save(model.state_dict(), expt_folder + \"SarcasmModel.pt\")\n",
        "        print(\"Saved Model\")\n",
        "\n",
        "  model.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  "
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaModel: ['lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
            "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2110: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration: 0 Loss: tensor(0.6860, device='cuda:0')\n",
            "Batch Accuracy: tensor(50.5546, device='cuda:0')\n",
            "Saved Model\n",
            "Iteration: 320 Loss: tensor(0.6877, device='cuda:0')\n",
            "Batch Accuracy: tensor(50.2874, device='cuda:0')\n",
            "Iteration: 640 Loss: tensor(0.6891, device='cuda:0')\n",
            "Batch Accuracy: tensor(50.2291, device='cuda:0')\n",
            "Iteration: 960 Loss: tensor(0.6063, device='cuda:0')\n",
            "Batch Accuracy: tensor(56.3638, device='cuda:0')\n",
            "Iteration: 1280 Loss: tensor(0.4533, device='cuda:0')\n",
            "Batch Accuracy: tensor(64.5739, device='cuda:0')\n",
            "Iteration: 1600 Loss: tensor(0.4918, device='cuda:0')\n",
            "Batch Accuracy: tensor(64.8719, device='cuda:0')\n",
            "Iteration: 1920 Loss: tensor(0.5194, device='cuda:0')\n",
            "Batch Accuracy: tensor(63.3478, device='cuda:0')\n",
            "Iteration: 2240 Loss: tensor(0.4883, device='cuda:0')\n",
            "Batch Accuracy: tensor(67.1954, device='cuda:0')\n",
            "Iteration: 2560 Loss: tensor(0.2356, device='cuda:0')\n",
            "Batch Accuracy: tensor(82.3740, device='cuda:0')\n",
            "Iteration: 2880 Loss: tensor(0.3126, device='cuda:0')\n",
            "Batch Accuracy: tensor(81.2447, device='cuda:0')\n",
            "Iteration: 3200 Loss: tensor(0.3100, device='cuda:0')\n",
            "Batch Accuracy: tensor(79.8432, device='cuda:0')\n",
            "Iteration: 3520 Loss: tensor(0.2758, device='cuda:0')\n",
            "Batch Accuracy: tensor(79.5376, device='cuda:0')\n",
            "Iteration: 3840 Loss: tensor(0.4316, device='cuda:0')\n",
            "Batch Accuracy: tensor(72.6282, device='cuda:0')\n",
            "Iteration: 4160 Loss: tensor(0.3259, device='cuda:0')\n",
            "Batch Accuracy: tensor(77.9347, device='cuda:0')\n",
            "Iteration: 4480 Loss: tensor(0.4977, device='cuda:0')\n",
            "Batch Accuracy: tensor(76.3523, device='cuda:0')\n",
            "Iteration: 4800 Loss: tensor(0.2341, device='cuda:0')\n",
            "Batch Accuracy: tensor(82.9187, device='cuda:0')\n",
            "Iteration: 5120 Loss: tensor(0.3392, device='cuda:0')\n",
            "Batch Accuracy: tensor(80.2809, device='cuda:0')\n",
            "Saved Model\n",
            "Iteration: 5440 Loss: tensor(0.2823, device='cuda:0')\n",
            "Batch Accuracy: tensor(79.8418, device='cuda:0')\n",
            "Iteration: 5760 Loss: tensor(0.3964, device='cuda:0')\n",
            "Batch Accuracy: tensor(83.0087, device='cuda:0')\n",
            "Iteration: 6080 Loss: tensor(0.4309, device='cuda:0')\n",
            "Batch Accuracy: tensor(78.5441, device='cuda:0')\n",
            "Iteration: 6400 Loss: tensor(0.2830, device='cuda:0')\n",
            "Batch Accuracy: tensor(79.6301, device='cuda:0')\n",
            "Iteration: 6720 Loss: tensor(0.2137, device='cuda:0')\n",
            "Batch Accuracy: tensor(83.9693, device='cuda:0')\n",
            "Iteration: 7040 Loss: tensor(0.2686, device='cuda:0')\n",
            "Batch Accuracy: tensor(83.2916, device='cuda:0')\n",
            "Iteration: 7360 Loss: tensor(0.2166, device='cuda:0')\n",
            "Batch Accuracy: tensor(82.5926, device='cuda:0')\n",
            "Iteration: 7680 Loss: tensor(0.2282, device='cuda:0')\n",
            "Batch Accuracy: tensor(83.9588, device='cuda:0')\n",
            "Iteration: 8000 Loss: tensor(0.4163, device='cuda:0')\n",
            "Batch Accuracy: tensor(74.8983, device='cuda:0')\n",
            "Iteration: 8320 Loss: tensor(0.3784, device='cuda:0')\n",
            "Batch Accuracy: tensor(84.6351, device='cuda:0')\n",
            "Iteration: 8640 Loss: tensor(0.2205, device='cuda:0')\n",
            "Batch Accuracy: tensor(85.1691, device='cuda:0')\n",
            "Iteration: 8960 Loss: tensor(0.2355, device='cuda:0')\n",
            "Batch Accuracy: tensor(85.1688, device='cuda:0')\n",
            "Iteration: 9280 Loss: tensor(0.2295, device='cuda:0')\n",
            "Batch Accuracy: tensor(84.3153, device='cuda:0')\n",
            "Iteration: 9600 Loss: tensor(0.1730, device='cuda:0')\n",
            "Batch Accuracy: tensor(89.3455, device='cuda:0')\n",
            "Iteration: 9920 Loss: tensor(0.3911, device='cuda:0')\n",
            "Batch Accuracy: tensor(84.2349, device='cuda:0')\n",
            "Iteration: 10240 Loss: tensor(0.1859, device='cuda:0')\n",
            "Batch Accuracy: tensor(87.3177, device='cuda:0')\n",
            "Saved Model\n",
            "Iteration: 10560 Loss: tensor(0.3042, device='cuda:0')\n",
            "Batch Accuracy: tensor(82.1472, device='cuda:0')\n",
            "Iteration: 10880 Loss: tensor(0.1564, device='cuda:0')\n",
            "Batch Accuracy: tensor(87.4648, device='cuda:0')\n",
            "Iteration: 11200 Loss: tensor(0.1525, device='cuda:0')\n",
            "Batch Accuracy: tensor(88.0037, device='cuda:0')\n",
            "Iteration: 11520 Loss: tensor(0.3084, device='cuda:0')\n",
            "Batch Accuracy: tensor(80.3485, device='cuda:0')\n",
            "Iteration: 11840 Loss: tensor(0.2815, device='cuda:0')\n",
            "Batch Accuracy: tensor(84.9724, device='cuda:0')\n",
            "Iteration: 12160 Loss: tensor(0.1780, device='cuda:0')\n",
            "Batch Accuracy: tensor(87.5730, device='cuda:0')\n",
            "Iteration: 12480 Loss: tensor(0.2826, device='cuda:0')\n",
            "Batch Accuracy: tensor(85.8774, device='cuda:0')\n",
            "Iteration: 12800 Loss: tensor(0.2161, device='cuda:0')\n",
            "Batch Accuracy: tensor(83.2413, device='cuda:0')\n",
            "Iteration: 13120 Loss: tensor(0.4928, device='cuda:0')\n",
            "Batch Accuracy: tensor(78.8628, device='cuda:0')\n",
            "Iteration: 13440 Loss: tensor(0.2873, device='cuda:0')\n",
            "Batch Accuracy: tensor(77.9447, device='cuda:0')\n",
            "Iteration: 13760 Loss: tensor(0.3533, device='cuda:0')\n",
            "Batch Accuracy: tensor(78.0118, device='cuda:0')\n",
            "Iteration: 14080 Loss: tensor(0.3461, device='cuda:0')\n",
            "Batch Accuracy: tensor(85.9535, device='cuda:0')\n",
            "Iteration: 14400 Loss: tensor(0.2285, device='cuda:0')\n",
            "Batch Accuracy: tensor(84.9789, device='cuda:0')\n",
            "Iteration: 14720 Loss: tensor(0.3038, device='cuda:0')\n",
            "Batch Accuracy: tensor(82.1595, device='cuda:0')\n",
            "Iteration: 15040 Loss: tensor(0.2087, device='cuda:0')\n",
            "Batch Accuracy: tensor(85.6984, device='cuda:0')\n",
            "Iteration: 15360 Loss: tensor(0.1324, device='cuda:0')\n",
            "Batch Accuracy: tensor(91.4453, device='cuda:0')\n",
            "Saved Model\n",
            "Iteration: 15680 Loss: tensor(0.2535, device='cuda:0')\n",
            "Batch Accuracy: tensor(86.3154, device='cuda:0')\n",
            "Iteration: 16000 Loss: tensor(0.1844, device='cuda:0')\n",
            "Batch Accuracy: tensor(86.1181, device='cuda:0')\n",
            "Iteration: 16320 Loss: tensor(0.1454, device='cuda:0')\n",
            "Batch Accuracy: tensor(88.3210, device='cuda:0')\n",
            "Iteration: 16640 Loss: tensor(0.1382, device='cuda:0')\n",
            "Batch Accuracy: tensor(90.0413, device='cuda:0')\n",
            "Iteration: 16960 Loss: tensor(0.1210, device='cuda:0')\n",
            "Batch Accuracy: tensor(90.2567, device='cuda:0')\n",
            "Iteration: 17280 Loss: tensor(0.0932, device='cuda:0')\n",
            "Batch Accuracy: tensor(92.9227, device='cuda:0')\n",
            "Iteration: 17600 Loss: tensor(0.3151, device='cuda:0')\n",
            "Batch Accuracy: tensor(81.5299, device='cuda:0')\n",
            "Iteration: 17920 Loss: tensor(0.5439, device='cuda:0')\n",
            "Batch Accuracy: tensor(78.0204, device='cuda:0')\n",
            "Iteration: 18240 Loss: tensor(0.1592, device='cuda:0')\n",
            "Batch Accuracy: tensor(87.8144, device='cuda:0')\n",
            "Iteration: 18560 Loss: tensor(0.4512, device='cuda:0')\n",
            "Batch Accuracy: tensor(79.3488, device='cuda:0')\n",
            "Iteration: 18880 Loss: tensor(0.4244, device='cuda:0')\n",
            "Batch Accuracy: tensor(78.7853, device='cuda:0')\n",
            "Iteration: 19200 Loss: tensor(0.0932, device='cuda:0')\n",
            "Batch Accuracy: tensor(91.8922, device='cuda:0')\n",
            "Iteration: 19520 Loss: tensor(0.1306, device='cuda:0')\n",
            "Batch Accuracy: tensor(89.7077, device='cuda:0')\n",
            "Iteration: 19840 Loss: tensor(0.2333, device='cuda:0')\n",
            "Batch Accuracy: tensor(85.0180, device='cuda:0')\n",
            "Iteration: 20160 Loss: tensor(0.3235, device='cuda:0')\n",
            "Batch Accuracy: tensor(80.7972, device='cuda:0')\n",
            "Iteration: 20480 Loss: tensor(0.2444, device='cuda:0')\n",
            "Batch Accuracy: tensor(82.6416, device='cuda:0')\n",
            "Saved Model\n",
            "Iteration: 20800 Loss: tensor(0.3585, device='cuda:0')\n",
            "Batch Accuracy: tensor(83.2582, device='cuda:0')\n",
            "Iteration: 21120 Loss: tensor(0.4541, device='cuda:0')\n",
            "Batch Accuracy: tensor(80.7414, device='cuda:0')\n",
            "Iteration: 21440 Loss: tensor(0.0756, device='cuda:0')\n",
            "Batch Accuracy: tensor(93.2132, device='cuda:0')\n",
            "Iteration: 21760 Loss: tensor(0.2629, device='cuda:0')\n",
            "Batch Accuracy: tensor(84.4874, device='cuda:0')\n",
            "Iteration: 22080 Loss: tensor(0.2442, device='cuda:0')\n",
            "Batch Accuracy: tensor(88.7013, device='cuda:0')\n",
            "Iteration: 22400 Loss: tensor(0.0686, device='cuda:0')\n",
            "Batch Accuracy: tensor(94.0560, device='cuda:0')\n",
            "Iteration: 22720 Loss: tensor(0.1183, device='cuda:0')\n",
            "Batch Accuracy: tensor(91.6754, device='cuda:0')\n",
            "Iteration: 23040 Loss: tensor(0.1062, device='cuda:0')\n",
            "Batch Accuracy: tensor(92.1592, device='cuda:0')\n",
            "Iteration: 23360 Loss: tensor(0.2563, device='cuda:0')\n",
            "Batch Accuracy: tensor(86.8346, device='cuda:0')\n",
            "Iteration: 23680 Loss: tensor(0.1994, device='cuda:0')\n",
            "Batch Accuracy: tensor(84.7975, device='cuda:0')\n",
            "Iteration: 24000 Loss: tensor(0.0910, device='cuda:0')\n",
            "Batch Accuracy: tensor(92.0154, device='cuda:0')\n",
            "Iteration: 24320 Loss: tensor(0.1436, device='cuda:0')\n",
            "Batch Accuracy: tensor(89.4023, device='cuda:0')\n",
            "Iteration: 24640 Loss: tensor(0.2174, device='cuda:0')\n",
            "Batch Accuracy: tensor(88.7360, device='cuda:0')\n",
            "Iteration: 24960 Loss: tensor(0.2064, device='cuda:0')\n",
            "Batch Accuracy: tensor(84.4997, device='cuda:0')\n",
            "Iteration: 25280 Loss: tensor(0.2561, device='cuda:0')\n",
            "Batch Accuracy: tensor(84.0840, device='cuda:0')\n",
            "Iteration: 25600 Loss: tensor(0.2457, device='cuda:0')\n",
            "Batch Accuracy: tensor(82.4549, device='cuda:0')\n",
            "Saved Model\n",
            "Iteration: 25920 Loss: tensor(0.3161, device='cuda:0')\n",
            "Batch Accuracy: tensor(78.9414, device='cuda:0')\n",
            "Iteration: 26240 Loss: tensor(0.4009, device='cuda:0')\n",
            "Batch Accuracy: tensor(83.1537, device='cuda:0')\n",
            "Iteration: 26560 Loss: tensor(0.1568, device='cuda:0')\n",
            "Batch Accuracy: tensor(88.1966, device='cuda:0')\n",
            "Iteration: 26880 Loss: tensor(0.1263, device='cuda:0')\n",
            "Batch Accuracy: tensor(90.6749, device='cuda:0')\n",
            "Iteration: 27200 Loss: tensor(0.2046, device='cuda:0')\n",
            "Batch Accuracy: tensor(85.9515, device='cuda:0')\n",
            "Iteration: 27520 Loss: tensor(0.0933, device='cuda:0')\n",
            "Batch Accuracy: tensor(92.5661, device='cuda:0')\n",
            "Iteration: 27840 Loss: tensor(0.3199, device='cuda:0')\n",
            "Batch Accuracy: tensor(81.0740, device='cuda:0')\n",
            "Iteration: 28160 Loss: tensor(0.2596, device='cuda:0')\n",
            "Batch Accuracy: tensor(86.2376, device='cuda:0')\n",
            "Iteration: 28480 Loss: tensor(0.1332, device='cuda:0')\n",
            "Batch Accuracy: tensor(89.1230, device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K1OPRtTRxkWo"
      },
      "source": [
        "def predict(b_headline, tokenizer, model):\n",
        "  b_input_ids, b_attention_mask = get_BERT_input(b_headline, tokenizer)\n",
        "  b_predictions = model(b_input_ids.to(device), b_attention_mask.to(device))\n",
        "  sarcastic_probability = b_predictions.data[0][1].item() * 100\n",
        "  not_sarcastic_probability = b_predictions.data[0][0].item() * 100\n",
        "  print_string = \"Sarcastic:\", f'{sarcastic_probability:.2f}', \"Not Sarcastic:\", f'{not_sarcastic_probability:.2f}'\n",
        "  # return b_predictions\n",
        "  return print_string"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2hdn6UN82PmU",
        "outputId": "6cb0f382-9acc-49ba-f2e5-a00cad1d733a"
      },
      "source": [
        "predict([\"Õ¥Õ½ Õ½Õ«Ö€Õ¸Ö‚Õ´ Õ¥Õ´ Õ´Õ¡ÕµÖ€Õ«Õ¯Õ«Õ½\"],tokenizer, model)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2110: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('Sarcastic:', '2.08', 'Not Sarcastic:', '97.92')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    }
  ]
}